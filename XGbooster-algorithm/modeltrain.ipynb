{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   }
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "#%load process.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.set_printoptions(suppress=True)\n",
    "import sys\n",
    "import argparse\n",
    "\n",
    "expid=2222\n",
    "n=10\n",
    "hayrows=1000\n",
    "# needlerows=150000\n",
    "hayfill=99\n",
    "md=20\n",
    "nr=400\n",
    "\n",
    "\n",
    "infinityfill=999999\n",
    "nafill=0\n",
    "\n",
    "filename='modeltest1'+str(expid)\n",
    "##############################################################\n",
    "\n",
    "hay=pd.read_csv('200k.haystack',low_memory=False,nrows=hayrows)\n",
    "\n",
    "haynames=hay.index\n",
    "\n",
    "needle=pd.read_csv('203.needle',nrows=1000)# ,low_memory=False,nrows=needlerows) #read in the actual needles from the desc\n",
    "\n",
    "needlenames=needle.index\n",
    "\n",
    "needleflag=[1]*len(needlenames)\n",
    "hayflag=[0]*len(haynames)\n",
    "\n",
    "hay.columns=needle.columns\n",
    "train=needle.append([hay])\n",
    "del hay\n",
    "#del needle\n",
    "train=train.reset_index(drop=True)\n",
    "\n",
    "#hayneedlenames=train['Name']\n",
    "#train=train.drop('Name',axis=1)\n",
    "\n",
    "######### cleaning\n",
    "import re\n",
    "print(train.shape)\n",
    "\n",
    "train=train.replace({'Infinity': infinityfill}, regex=True)\n",
    "\n",
    "train=train.fillna(nafill)\n",
    "    # train=train.replace({'#NAME?': '9999'}, regex=True)\n",
    "    # train=train.replace({'9999': 9999}, regex=True)\n",
    "    #train['row']=train.index.values #adds a new column which is just the order of stuff-- this is the BAD thing that fucks up the data\n",
    "\n",
    "X=train.copy(deep=True)\n",
    "# del train\n",
    "\n",
    "y=pd.DataFrame(needleflag)\n",
    "y1=pd.DataFrame(hayflag)\n",
    "y=y.append([y1])\n",
    "y=y.reset_index(drop=True)\n",
    "# y=y.rename(columns={0:'assay'}, inplace=True)\n",
    "y.rename(columns={0:'assay'}, inplace=True)\n",
    "\n",
    "# height = hayrows\n",
    "# width = 1\n",
    "# z = pd.DataFrame(hayfill, index=range(height), columns=range(width))\n",
    "# z.columns = ['assay']\n",
    "\n",
    "\n",
    "print(y.shape)\n",
    "print(X.shape)\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "selector = VarianceThreshold(threshold=0.3)\n",
    "selector.fit(X)\n",
    "X=X[X.columns[selector.get_support(indices=True)]] #this is the data, but without the weights-- IT DROPS A BUNCH OF THE COLUMNS\n",
    "\n",
    "\n",
    " \n",
    "import time\n",
    "import xgboost as xgb \n",
    "from sklearn.metrics import log_loss\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import heapq\n",
    "import random\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    " \n",
    " \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "y=pd.DataFrame(y)\n",
    "#y.columns=['target']\n",
    "from sklearn.preprocessing import  StandardScaler\n",
    "sc=StandardScaler()\n",
    "names=list(X.columns)\n",
    "X=sc.fit_transform(X)\n",
    "X=pd.DataFrame(X)\n",
    "X.columns=names\n",
    "drop_list = list(set(train.columns)-set(names))\n",
    "for col in drop_list:\n",
    "    train.drop(col,axis=1,inplace=True)\n",
    "    pass\n",
    "train.columns=names\n",
    "X = train.reset_index(drop=True)\n",
    "#X.to_csv('X.csv',index=False)\n",
    "#X_train.to_csv('Xtrain.csv',index=False)\n",
    "#y_train.to_csv('ytrain.csv',index=False)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.4, random_state=0)\n",
    "\n",
    "\n",
    "# ytrainsid=y_train['SID']\n",
    "# ytrainsid=pd.DataFrame(ytrainsid)\n",
    "\n",
    "# ytestsid=y_test['SID']\n",
    "# ytestsid=pd.DataFrame(ytestsid)\n",
    "\n",
    "\n",
    "\n",
    "# y_train=y_train.drop('SID', axis=1)\n",
    "# y_test=y_test.drop('SID', axis=1)\n",
    "\n",
    "#row_number=X_test.index.values.astype(int) #this is the virulent code\n",
    "\n",
    "########################################################################################################### END of my edits part 1\n",
    "\n",
    "###this is the part that is in teh basic xgb\n",
    "dtrain = xgb.DMatrix(data=X_train.values,feature_names=X_train.columns,label=y_train.values ) \n",
    "dtest = xgb.DMatrix(data=X_test.values,feature_names=X_test.columns)\n",
    "np.set_printoptions(suppress=True)\n",
    " \n",
    "del X_train\n",
    "del X_test\n",
    "del X\n",
    "\n",
    "\n",
    "part1= time.time() - start_time\n",
    "part1\n",
    "\n",
    "start_time = time.time()\n",
    "file=str(md)+'-'+str(nr)+'-'+str(n)+'n'+str(hayrows)+'for-model-gen-for-first-use'\n",
    "\n",
    "\n",
    "def create_individual():\n",
    "    \"\"\"creates random xgboost  models\"\"\"\n",
    "    import xgboost as xgb\n",
    "    param ={ \n",
    "                            'max_depth': int(random.randrange(1,int(md) , step= 1)),#should be 20\n",
    "                            'num_round':int(random.randrange(1, int(nr))),#should be 400\n",
    "                            'child_wght' :round(random.uniform(0.01, 10.0), 2),\n",
    "                            'learning_rate':round(random.uniform(0.01, 1), 1),\n",
    "                            'colsample_bytree':round(random.uniform(0.01, 1.0), 2),\n",
    "                            \n",
    "                            'tree_method':'gpu_hist',\n",
    "                            'n_gpus': -1,\n",
    "                            # 'objective': 'reg:linear',\n",
    "                            'objective': 'binary:logistic',                         \n",
    "                             \n",
    "                            }\n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                           \n",
    "    return param\n",
    " \n",
    "\n",
    "import random\n",
    "#n=100 #number of models to be created\n",
    "stone_age=[create_individual() for i in range(n)]\n",
    "\n",
    "asdf=y_test['assay']\n",
    "asdf=pd.DataFrame(asdf)\n",
    "import datetime\n",
    "\n",
    "def fraud_fitness(model ,dtrain,dtest,y_test):\n",
    "        param=create_individual()\n",
    "        num_round=param['num_round']\n",
    "       #num_round=random.randrange(10, 100)\n",
    "        bst=xgb.train(param, dtrain,num_round)\n",
    "        print(bst)\n",
    "        joblib.dump(bst, filename+str(count)+'.mod3') #my addition-- dumps the model for each iteration\n",
    "        pred=bst.predict(dtest,ntree_limit=bst.best_iteration)\n",
    "        \n",
    "#        print(pred)\n",
    "#        print(pred.shape)\n",
    "\n",
    "        DT = datetime.datetime.now()\n",
    "        nam=str(DT.microsecond)\n",
    "        asdf[nam]=pred \n",
    "       \n",
    "        \n",
    "        from sklearn.metrics  import mean_squared_error\n",
    "#         print('y_test')\n",
    "#         print(y_test)\n",
    "#         print('pred')    \n",
    "#         print(pred)    \n",
    "#        global predtest\n",
    "#        predtest=pred\n",
    "        \n",
    "       #fixing b to be like i need it to be\n",
    "        datain=pd.DataFrame(y_test)\n",
    "        datain=datain.reset_index(drop=True)\n",
    "        datain['pred']=pd.DataFrame(pred)\n",
    "        dataout=datain.copy(deep=True)\n",
    "        for col in dataout.columns:\n",
    "            dataout[col].values[:] = 0\n",
    "        threshold=.9\n",
    "        index=datain.columns[1]\n",
    "        dataout[index]=np.where(datain[index]>threshold, 1,0)    \n",
    "        dataout['assay']=datain['assay']\n",
    "        dataout[index]=np.where(dataout[index]==1,dataout['assay']+100,0)+np.where(dataout[index]==0,dataout['assay']+200,0)\n",
    "        tempframe = pd.DataFrame(    [[        0,1,2,3,4    ]])\n",
    "        tempframe = tempframe.append(        pd.DataFrame(        [[            index,(dataout[index]==101).sum(),(dataout[index]==100).sum(),(dataout[index]==201).sum(),(dataout[index]==200).sum()        ]]        )    )\n",
    "        tempframe['%ofhits_found']=tempframe[1]/(tempframe[1]+tempframe[3])\n",
    "        tempframe['poshits_that_are_pos']=tempframe[1]/(tempframe[1]+tempframe[2])\n",
    "        tempframe['neghits_that_are_neg']=tempframe[4]/(tempframe[3]+tempframe[4])\n",
    "        tempframe['score']=tempframe['%ofhits_found']+tempframe['poshits_that_are_pos']\n",
    "        tempframe=tempframe.set_index(0)\n",
    "        tempframe=tempframe.drop(tempframe.index[0])\n",
    "        tempframe=tempframe.sort_values(by=['score'])\n",
    "#         print(tempframe)\n",
    "        b=tempframe['score'].item() \n",
    "        del tempframe\n",
    "        del datain \n",
    "        del dataout\n",
    "        \n",
    "#         b=mean_squared_error(y_test,pred)\n",
    "        \n",
    "        return    b\n",
    "\n",
    "count=1\n",
    "\n",
    "class generation(object):\n",
    "    \n",
    "    def __init__(self,p):\n",
    "        \"\"\"here  the  class is initialized with initial(stone age population) . Meaning\n",
    "        random   neural net arhitectures or  xgb\"\"\"\n",
    "        self.gen=p\n",
    "        self.perf=[]\n",
    "    \n",
    "    def fitness(self):\n",
    "        global count\n",
    "        for model in self.gen:\n",
    "            b=fraud_fitness(model,dtrain,dtest,y_test)\n",
    "            print('b is'+str(b))\n",
    "            self.perf.append(b)\n",
    "            count=count+1\n",
    "            print(count)\n",
    "        return self.gen\n",
    "\n",
    "s=generation(stone_age)\n",
    "\n",
    "s.fitness()\n",
    "\n",
    "part2= time.time() - start_time\n",
    "part2\n",
    "\n",
    "\n",
    "asdf.to_csv(file+'.csv')\n",
    "\n",
    "tempperf=pd.DataFrame(s.perf)\n",
    "tempperf.to_csv(file+'.perf') #this is the mean squared error\n",
    "\n",
    "f = open( file+'.time', 'w' )\n",
    "f.write( str(part1)+','+str(part2) )\n",
    "f.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ]
}